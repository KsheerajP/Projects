# AI-Based Emotion Detection and Music Selection System

## Overview
This project implements a multi-modal emotion recognition system that analyzes user inputs from facial expressions, voice signals, and text to determine emotional state and suggests music accordingly. It aims to enhance user well-being and personalization in music playback using deep learning and psychological emotion-music mapping models.

## Accessing the Code

If you're interested in exploring the full complete code of this project, feel free to reach out to me via 
[LinkedIn](https://www.linkedin.com/in/ksheerajprakash) or email me at `ksheerooo@gmail.com`. Iâ€™ll be happy to provide access upon request.

ðŸ”— GitHub Repository (private): [https://github.com/KsheerajP/Emotion-Detection-and-Music-Selection.git](https://github.com/KsheerajP/Emotion-Detection-and-Music-Selection.git)

## How It Works
The system takes in three types of inputs from the user:
- **Facial Expressions** â€“ captured through a webcam or image to sense visible emotions.
- **Voice** â€“ short voice samples are used to assess tone and mood.
- **Text** â€“ user-typed input is scanned for emotion-related cues.

These inputs are processed to determine whether the person is feeling happy, sad, angry, relaxed, etc. Based on this detected emotional state, the system automatically recommends a matching genre or playlist.

Itâ€™s also built to work in real time and can run offline once set up.

## Results
- The system achieved **up to 88% accuracy** in detecting emotional states from the different input types.
- The overall **F1-score is 0.85**, showing balanced performance across different emotion categories.
- In user testing, the music suggestions were found to be **highly relevant** to the actual emotions, improving user satisfaction and personalization.

